{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FluxMPI.jl: Distributed Data Parallel Training of Neural Networks","text":""},{"location":"#fluxmpijl","title":"FluxMPI.jl","text":"<p>Distributed Data Parallel Training of Neural Networks</p> <p>Note</p> <p>This package has very little to do with Flux. FWIW it doesn't even depend on it. It can be seamlessly used with Flux.jl, Lux.jl, and pretty much any framework which works with Optimisers.jl.</p> <p></p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Install julia v1.6 or above. Next Install the stable release:</p> <pre><code>using Pkg\nPkg.add(\"FluxMPI\")\n</code></pre> <p>To install the Latest development version (not very beneficial we release most patches almost immediately):</p> <pre><code>using Pkg\nPkg.add(\"FluxMPI\", rev=\"main\")\n</code></pre> <p></p> <p></p>"},{"location":"#design-principles","title":"Design Principles","text":"<ul> <li>Efficient distributed data parallelism using MPI.</li> <li>Not tied to any specific framework \u2013 works with Lux.jl, Flux.jl, etc.</li> <li>Should not be too intrusive.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you found this library to be useful in academic work, then please cite:</p> <pre><code>@misc{pal2022lux,\nauthor = {Pal, Avik},\ntitle = {FluxMPI: Distributed Data Parallel Training of Neural Networks},\nyear = {2021},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/avik-pal/FluxMPI.jl/}}\n}\n</code></pre> <p>Also consider starring our github repo</p>"},{"location":"api/","title":"API Reference","text":"<ul> <li><code>FluxMPI.DistributedDataContainer</code></li> <li><code>FluxMPI.DistributedOptimizer</code></li> <li><code>FluxMPI.Iallreduce!</code></li> <li><code>FluxMPI.Ibcast!</code></li> <li><code>FluxMPI.Init</code></li> <li><code>FluxMPI.Initialized</code></li> <li><code>FluxMPI.allreduce!</code></li> <li><code>FluxMPI.allreduce_gradients</code></li> <li><code>FluxMPI.bcast!</code></li> <li><code>FluxMPI.disable_cudampi_support</code></li> <li><code>FluxMPI.fluxmpi_print</code></li> <li><code>FluxMPI.fluxmpi_println</code></li> <li><code>FluxMPI.local_rank</code></li> <li><code>FluxMPI.reduce!</code></li> <li><code>FluxMPI.synchronize!</code></li> <li><code>FluxMPI.total_workers</code></li> </ul>"},{"location":"api/#data-helpers","title":"Data Helpers","text":"<p># <code>FluxMPI.DistributedDataContainer</code> \u2014 Type.</p> <pre><code>DistributedDataContainer(data)\n</code></pre> <p><code>data</code> must be compatible with <code>MLUtils</code> interface. The returned container is compatible with <code>MLUtils</code> interface and is used to partition the dataset across the available processes.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/#general-functions","title":"General Functions","text":"<p># <code>FluxMPI.Init</code> \u2014 Function.</p> <pre><code>Init(; gpu_devices::Union{Nothing,Vector{Int}}=nothing, verbose::Bool=false)\n</code></pre> <p>Setup <code>FluxMPI</code>. If GPUs are available and CUDA is functional, each rank is allocated a GPU in a round-robin fashion.</p> <p>If calling this function, no need to call <code>MPI.Init</code> first.</p> <p>source</p> <p># <code>FluxMPI.Initialized</code> \u2014 Function.</p> <pre><code>Initialized()\n</code></pre> <p>Has FluxMPI been initialized?</p> <p>source</p> <p># <code>FluxMPI.fluxmpi_print</code> \u2014 Function.</p> <pre><code>fluxmpi_print(args...; kwargs...)\n</code></pre> <p>Add <code>rank</code> and <code>size</code> information to the printed statement</p> <p>source</p> <p># <code>FluxMPI.fluxmpi_println</code> \u2014 Function.</p> <pre><code>fluxmpi_println(args...; kwargs...)\n</code></pre> <p>Add <code>rank</code> and <code>size</code> information to the printed statement</p> <p>source</p> <p># <code>FluxMPI.local_rank</code> \u2014 Function.</p> <pre><code>local_rank()\n</code></pre> <p>Get the rank of the process.</p> <p>source</p> <p># <code>FluxMPI.total_workers</code> \u2014 Function.</p> <pre><code>total_workers()\n</code></pre> <p>Get the total number of workers.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/#mpiextensions-blocking-communication-wrappers","title":"MPIExtensions: Blocking Communication Wrappers","text":"<p># <code>FluxMPI.allreduce!</code> \u2014 Function.</p> <pre><code>allreduce!(v, op, comm)\n</code></pre> <p>Perform <code>MPI.Allreduce!</code> ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled.</p> <p>source</p> <p># <code>FluxMPI.bcast!</code> \u2014 Function.</p> <pre><code>bcast!(v, op, comm)\n</code></pre> <p>Perform <code>MPI.Bcast!</code> ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled.</p> <p>source</p> <p># <code>FluxMPI.reduce!</code> \u2014 Function.</p> <pre><code>reduce!(v, op, comm)\n</code></pre> <p>Perform <code>MPI.Reduce!</code> ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/#mpiextensions-non-blocking-communication","title":"MPIExtensions: Non-Blocking Communication","text":"<p># <code>FluxMPI.Iallreduce!</code> \u2014 Function.</p> <pre><code>Iallreduce!(sendbuf, recvbuf, op, comm)\nIallreduce!(sendrecvbuf, op, comm)\n</code></pre> <p>Performs non-blocking elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>. <code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p> <p><code>recvbuf</code> and an MPI_Request object are returned. The value in <code>recvbuf</code> is only valid after the request has been completed. (<code>MPI.Wait!</code>)</p> <p>Warning</p> <p>OpenMPI doesn't support Iallreduce! with CUDA. See this issue.</p> <p>source</p> <p># <code>FluxMPI.Ibcast!</code> \u2014 Function.</p> <pre><code>Ibcast!(buf, root, comm)\n</code></pre> <p>Non-blocking broadcast of the buffer <code>buf</code> to all processes in <code>comm</code>.</p> <p><code>recvbuf</code> and an MPI_Request object are returned. The value in <code>recvbuf</code> is only valid after the request has been completed. (<code>MPI.Wait!</code>)</p> <p>source</p> <p></p> <p></p>"},{"location":"api/#optimization","title":"Optimization","text":"<p># <code>FluxMPI.DistributedOptimizer</code> \u2014 Type.</p> <pre><code>DistributedOptimizer(optimizer)\n</code></pre> <p>Wrap the <code>optimizer</code> in a <code>DistributedOptimizer</code>. Before updating the parameters, this adds the gradients across the processes using non-blocking Allreduce</p> <p>Arguments</p> <ul> <li><code>optimizer</code>: An Optimizer compatible with the Optimisers.jl package</li> </ul> <p>Note</p> <p>Remember to scale the loss function by <code>1 / total_workers()</code> to ensure that the gradients are correctly averaged</p> <p>source</p> <p># <code>FluxMPI.allreduce_gradients</code> \u2014 Function.</p> <pre><code>allreduce_gradients(gs::NamedTuple; on_gpu::Bool=CUDA.functional())\n</code></pre> <p>Allreduce the gradients. This uses a non-blocking API which will be efficient for large containers of multiple parameter arrays.</p> <p>Arguments</p> <ul> <li><code>gs</code>: A <code>NamedTuple</code> of gradients</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>on_gpu</code>: Specify if the gradients are on GPU. Defaults to <code>CUDA.functional()</code></li> </ul> <p>Returns</p> <ul> <li><code>Allreduce</code>d NamedTuple of gradients</li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/#synchronization","title":"Synchronization","text":"<p># <code>FluxMPI.synchronize!</code> \u2014 Function.</p> <pre><code>synchronize!(x; root_rank::Integer=0)\n</code></pre> <p>Synchronize <code>x</code> across all processes.</p> <p>Note</p> <p>this function is not in-place for CuArrays when MPI is not CUDA aware.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/#configuration","title":"Configuration","text":"<p># <code>FluxMPI.disable_cudampi_support</code> \u2014 Function.</p> <pre><code>disable_cudampi_support(; disable=true)\n</code></pre> <p>Disable CUDA MPI support. Julia Session needs to be restarted for this to take effect.</p> <p>source</p>"},{"location":"common_gotchas/","title":"Common Gotchas","text":""},{"location":"common_gotchas/#cuda-aware-mpi","title":"CUDA-aware MPI","text":""},{"location":"common_gotchas/#setup","title":"Setup","text":"<p>OpenMPI has extensive instructions on building CUDA-aware MPI. Next rebuild MPI.jl using these instructions.</p> <p>Additionally, make sure to set <code>JULIA_CUDA_USE_MEMPOOL=none</code>.</p> <p></p> <p></p>"},{"location":"common_gotchas/#should-you-use-cuda-aware-mpi","title":"Should you use CUDA-aware MPI?","text":"<p>I would recommend not using this atm, since <code>JULIA_CUDA_USE_MEMPOOL=none</code> will severely slow down your code (~2-3x for most workloads I tested). Instead setup <code>MPI.jl</code> using you system provided MPI and execute <code>FluxMPI.disable_cudampi_support()</code>.</p>"},{"location":"guide/","title":"Usage Guide","text":""},{"location":"guide/#guide-to-integrating-fluxmpi-into-your-code","title":"Guide to integrating FluxMPI into your code","text":"<p>There are essentially 6 main steps to remember:</p> <ol> <li>Initialize FluxMPI <code>FluxMPI.Init()</code>.</li> <li>Sync Model Parameters and States <code>FluxMPI.synchronize!(ps; root_rank)</code>. (Remember to use <code>FluxMPIFluxModel</code> for Flux models.)</li> <li>Use <code>DistributedDataContainer</code> to distribute your data evenly across the processes. (Of course an alternative is to just manually partition your data.)</li> <li>Wrap the optimizer in <code>DistributedOptimizer</code> or call  <code>allreduce_gradients(gs::NamedTuple)</code> before eveery <code>Optimisers.update</code>.</li> <li>Sync the optimizer state across the processes <code>FluxMPI.synchronize!(opt_state; root_rank)</code>.</li> <li>Change logging code to check for <code>local_rank</code> == 0.</li> </ol> <p>Finally, start the code using <code>mpiexecjl -n &lt;np&gt; julia --project=. &lt;filename&gt;.jl</code></p>"},{"location":"examples/flux/","title":"Usage with Flux","text":""},{"location":"examples/flux/#usage-example-with-fluxjl","title":"Usage example with FLux.jl","text":"<p>Flux.jl is one of the defacto deep learning framework in Julia.</p> <p></p> <p></p>"},{"location":"examples/flux/#step-0-import-the-necessary-packages","title":"Step 0: Import the necessary packages","text":"<p>Tip</p> <p>You can install these packages using <code>using Pkg; Pkg.add.([\"CUDA\", \"Optimisers\", \"Flux\", \"Random\", \"Zygote\"])</code></p> <pre><code>using CUDA, Optimisers, FluxMPI, Flux, Random, Zygote\n</code></pre> <p></p> <p></p>"},{"location":"examples/flux/#step-1-initialize-fluxmpi-not-doing-this-will-segfault-your-code","title":"Step 1: Initialize FluxMPI. Not doing this will segfault your code","text":"<pre><code>FluxMPI.Init()\nCUDA.allowscalar(false)\n</code></pre>"},{"location":"examples/flux/#step-2-sync-model-parameters-and-states","title":"Step 2: Sync Model Parameters and States","text":"<pre><code>model = Chain(Dense(1 =&gt; 256, tanh), Dense(256 =&gt; 512, tanh), Dense(512 =&gt; 256, tanh),\nDense(256 =&gt; 1))\n\nrng = Random.default_rng()\nRandom.seed!(rng, local_rank())\n# Always remember to wrap the model in FluxMPIFluxModel\nmodel = FluxMPI.synchronize!(FluxMPIFluxModel(model); root_rank=0)\n</code></pre>"},{"location":"examples/flux/#step-3-ensure-data-is-properly-partitioned","title":"Step 3: Ensure data is properly partitioned","text":"<p>It is the user's responsibility to partition the data across the processes. In this case, we are training on a total of <code>16 * &lt;np&gt;</code> samples. Instead of manually partitioning the data, we can use <code>DistributedDataContainer</code> to partition the data.</p> <pre><code>x = rand(rng, 1, 16) |&gt; gpu\ny = x .^ 2\n</code></pre> <p></p> <p></p>"},{"location":"examples/flux/#step-4-wrap-the-optimizer-in-distributedoptimizer","title":"Step 4: Wrap the optimizer in <code>DistributedOptimizer</code>","text":"<p>Remember to scale the learning rate by the number of workers <code>total_workers</code>.</p> <pre><code>opt = DistributedOptimizer(Optimisers.Adam(0.001f0))\nst_opt = Optimisers.setup(opt, model)\n\nloss(model) = sum(abs2, model(x) .- y)\n</code></pre> <p></p> <p></p>"},{"location":"examples/flux/#step-5-synchronize-the-optimizer-state","title":"Step 5: Synchronize the optimizer state","text":"<pre><code>st_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)\n</code></pre>"},{"location":"examples/flux/#step-6-train-your-model","title":"Step 6: Train your model","text":"<p>Remember to print using <code>fluxmpi_println</code> or <code>fluxmpi_print</code>.</p> <pre><code>gs_ = gradient(loss, model)[1]\nOptimisers.update(st_opt, ps, gs_)\n\nt1 = time()\n\nfor epoch in 1:100\nglobal model, st_opt\nl, back = Zygote.pullback(loss, model)\nFluxMPI.fluxmpi_println(\"Epoch $epoch: Loss $l\")\ngs = back(one(l))[1]\nst_opt, model = Optimisers.update(st_opt, model, gs)\nend\n\nFluxMPI.fluxmpi_println(time() - t1)\n</code></pre> <p>Run the code using <code>mpiexecjl -n 3 julia --project=. &lt;filename&gt;.jl</code>.</p> <p></p> <p></p>"},{"location":"examples/flux/#complete-script","title":"Complete Script","text":"<pre><code>using CUDA, FluxMPI, Flux, Optimisers, Random, Zygote\n\nFluxMPI.Init()\nCUDA.allowscalar(false)\n\nmodel = Chain(Dense(1 =&gt; 256, tanh), Dense(256 =&gt; 512, tanh), Dense(512 =&gt; 256, tanh),\nDense(256 =&gt; 1)) |&gt; gpu\n\nrng = Random.default_rng()\nRandom.seed!(rng, local_rank())\nmodel = FluxMPI.synchronize!(FluxMPIFluxModel(model); root_rank=0)\n\nx = rand(rng, 1, 16) |&gt; gpu\ny = x .^ 2\n\nopt = DistributedOptimizer(Optimisers.Adam(0.001f0))\nst_opt = Optimisers.setup(opt, model)\n\nloss(model) = sum(abs2, model(x) .- y)\n\nst_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)\n\ngs_ = gradient(loss, model)[1]\nOptimisers.update(st_opt, ps, gs_)\n\nt1 = time()\n\nfor epoch in 1:100\nglobal model, st_opt\nl, back = Zygote.pullback(loss, model)\nFluxMPI.fluxmpi_println(\"Epoch $epoch: Loss $l\")\ngs = back(one(l))[1]\nst_opt, model = Optimisers.update(st_opt, model, gs)\nend\n\nFluxMPI.fluxmpi_println(time() - t1)\n</code></pre>"},{"location":"examples/lux/","title":"Usage with Lux","text":""},{"location":"examples/lux/#usage-example-with-luxjl","title":"Usage example with Lux.jl","text":"<p>Lux.jl is a deep learning framework which provides a functional design to implement neural networks in Julia. If you are not familiar with Lux, first check-out this tutorial before proceeding.</p> <p></p> <p></p>"},{"location":"examples/lux/#step-0-import-the-necessary-packages","title":"Step 0: Import the necessary packages","text":"<p>Tip</p> <p>You can install these packages using <code>using Pkg; Pkg.add.([\"CUDA\", \"Optimisers\", \"Lux\", \"Random\", \"Zygote\"])</code></p> <pre><code>using CUDA, Optimisers, FluxMPI, Lux, Random, Zygote\n</code></pre> <p></p> <p></p>"},{"location":"examples/lux/#step-1-initialize-fluxmpi-not-doing-this-will-segfault-your-code","title":"Step 1: Initialize FluxMPI. Not doing this will segfault your code","text":"<pre><code>FluxMPI.Init()\nCUDA.allowscalar(false)\n</code></pre>"},{"location":"examples/lux/#step-2-sync-model-parameters-and-states","title":"Step 2: Sync Model Parameters and States","text":"<pre><code>model = Chain(Dense(1 =&gt; 256, tanh), Dense(256 =&gt; 512, tanh), Dense(512 =&gt; 256, tanh),\nDense(256 =&gt; 1))\nrng = Random.default_rng()\nRandom.seed!(rng, local_rank())\nps, st = Lux.setup(rng, model) .|&gt; gpu\n\nps = FluxMPI.synchronize!(ps; root_rank = 0)\nst = FluxMPI.synchronize!(st; root_rank = 0)\n</code></pre>"},{"location":"examples/lux/#step-3-ensure-data-is-properly-partitioned","title":"Step 3: Ensure data is properly partitioned","text":"<p>It is the user's responsibility to partition the data across the processes. In this case, we are training on a total of <code>16 * &lt;np&gt;</code> samples. Instead of manually partitioning the data, we can use <code>DistributedDataContainer</code> to partition the data.</p> <pre><code>x = rand(rng, 1, 16) |&gt; gpu\ny = x .^ 2\n</code></pre> <p></p> <p></p>"},{"location":"examples/lux/#step-4-wrap-the-optimizer-in-distributedoptimizer","title":"Step 4: Wrap the optimizer in <code>DistributedOptimizer</code>","text":"<p>Remember to scale the learning rate by the number of workers <code>total_workers</code>.</p> <pre><code>opt = DistributedOptimizer(Adam(0.001f0))\nst_opt = Optimisers.setup(opt, ps)\n\nloss(p) = sum(abs2, model(x, p, st)[1] .- y)\n</code></pre> <p></p> <p></p>"},{"location":"examples/lux/#step-5-synchronize-the-optimizer-state","title":"Step 5: Synchronize the optimizer state","text":"<pre><code>st_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)\n</code></pre>"},{"location":"examples/lux/#step-6-train-your-model","title":"Step 6: Train your model","text":"<p>Remember to print using <code>fluxmpi_println</code> or <code>fluxmpi_print</code>.</p> <pre><code>gs_ = gradient(loss, ps)[1]\nOptimisers.update(st_opt, ps, gs_)\n\nt1 = time()\n\nfor epoch in 1:100\nglobal ps, st_opt\nl, back = Zygote.pullback(loss, ps)\nFluxMPI.fluxmpi_println(\"Epoch $epoch: Loss $l\")\ngs = back(one(l))[1]\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\nend\n\nFluxMPI.fluxmpi_println(time() - t1)\n</code></pre> <p>Run the code using <code>mpiexecjl -n 3 julia --project=. &lt;filename&gt;.jl</code>.</p> <p></p> <p></p>"},{"location":"examples/lux/#complete-script","title":"Complete Script","text":"<pre><code>using CUDA, FluxMPI, Lux, Optimisers, Random, Zygote\n\nFluxMPI.Init()\nCUDA.allowscalar(false)\n\nmodel = Chain(Dense(1 =&gt; 256, tanh), Dense(256 =&gt; 512, tanh), Dense(512 =&gt; 256, tanh),\nDense(256 =&gt; 1))\nrng = Random.default_rng()\nRandom.seed!(rng, local_rank())\nps, st = Lux.setup(rng, model) .|&gt; gpu\n\nps = FluxMPI.synchronize!(ps; root_rank = 0)\nst = FluxMPI.synchronize!(st; root_rank = 0)\n\nx = rand(rng, 1, 16) |&gt; gpu\ny = x .^ 2\n\nopt = DistributedOptimizer(Adam(0.001f0))\nst_opt = Optimisers.setup(opt, ps)\n\nloss(p) = sum(abs2, model(x, p, st)[1] .- y)\n\nst_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)\n\ngs_ = gradient(loss, ps)[1]\nOptimisers.update(st_opt, ps, gs_)\n\nt1 = time()\n\nfor epoch in 1:100\nglobal ps, st_opt\nl, back = Zygote.pullback(loss, ps)\nFluxMPI.fluxmpi_println(\"Epoch $epoch: Loss $l\")\ngs = back(one(l))[1]\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\nend\n\nFluxMPI.fluxmpi_println(time() - t1)\n</code></pre>"}]}