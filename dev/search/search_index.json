{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FluxMPI.jl \u00a4 Distributed Data Parallel Training of Neural Networks Note This package has very little to do with Flux. FWIW it doesn't even depend on it. It can be seamlessly used with Flux.jl , Lux.jl , and pretty much any framework which works with Optimisers.jl . Warning I will be changing the name of this package in the near future. (Probably something along DDPUtils.jl) Installation \u00a4 Install julia v1.6 or above . Next Install the stable release: import Pkg Pkg . add ( \"FluxMPI\" ) To install the Latest development version (not very beneficial we release most patches almost immediately): import Pkg Pkg . add ( \"FluxMPI\" , rev = \"main\" ) Design Principles \u00a4 Efficient distributed data parallelism using MPI. Not tied to any specific framework \u2013 works with Lux.jl , Flux.jl , etc. Should not be too intrusive. Citation \u00a4 If you found this library to be useful in academic work, then please cite: @misc { pal2022lux , author = {Pal, Avik} , title = {FluxMPI: Distributed Data Parallel Training of Neural Networks} , year = {2021} , publisher = {GitHub} , journal = {GitHub repository} , howpublished = {\\url{https://github.com/avik-pal/FluxMPI.jl/}} } Also consider starring our github repo","title":"FluxMPI.jl: Distributed Data Parallel Training of Neural Networks"},{"location":"#fluxmpijl","text":"Distributed Data Parallel Training of Neural Networks Note This package has very little to do with Flux. FWIW it doesn't even depend on it. It can be seamlessly used with Flux.jl , Lux.jl , and pretty much any framework which works with Optimisers.jl . Warning I will be changing the name of this package in the near future. (Probably something along DDPUtils.jl)","title":"FluxMPI.jl"},{"location":"#installation","text":"Install julia v1.6 or above . Next Install the stable release: import Pkg Pkg . add ( \"FluxMPI\" ) To install the Latest development version (not very beneficial we release most patches almost immediately): import Pkg Pkg . add ( \"FluxMPI\" , rev = \"main\" )","title":"Installation"},{"location":"#design-principles","text":"Efficient distributed data parallelism using MPI. Not tied to any specific framework \u2013 works with Lux.jl , Flux.jl , etc. Should not be too intrusive.","title":"Design Principles"},{"location":"#citation","text":"If you found this library to be useful in academic work, then please cite: @misc { pal2022lux , author = {Pal, Avik} , title = {FluxMPI: Distributed Data Parallel Training of Neural Networks} , year = {2021} , publisher = {GitHub} , journal = {GitHub repository} , howpublished = {\\url{https://github.com/avik-pal/FluxMPI.jl/}} } Also consider starring our github repo","title":"Citation"},{"location":"api/","text":"FluxMPI.DistributedDataContainer FluxMPI.DistributedOptimizer FluxMPI.Init FluxMPI.Initialized FluxMPI.MPIExtensions.Iallreduce! FluxMPI.MPIExtensions.Ibcast! FluxMPI.MPIExtensions.allreduce! FluxMPI.MPIExtensions.bcast! FluxMPI.MPIExtensions.reduce! FluxMPI.allreduce_gradients FluxMPI.clean_print FluxMPI.clean_println FluxMPI.local_rank FluxMPI.synchronize! FluxMPI.total_workers Data Helpers \u00a4 # FluxMPI.DistributedDataContainer \u2014 Type . DistributedDataContainer ( data ) data must be compatible with MLUtils interface. The returned container is compatible with MLUtils interface and is used to partition the dataset across the available processes. source General Functions \u00a4 # FluxMPI.Init \u2014 Function . Init (; gpu_devices :: Union { Nothing , Vector { Int }} = nothing , verbose :: Bool = false ) Setup FluxMPI . If GPUs are available and CUDA is functional, each rank is allocated a GPU in a round-robin fashion. If calling this function, no need to call MPI.Init first. source # FluxMPI.Initialized \u2014 Function . Initialized () Has FluxMPI been initialized? source # FluxMPI.clean_print \u2014 Function . clean_print ( args ... ; kwargs ... ) Add rank and size information to the printed statement source # FluxMPI.clean_println \u2014 Function . clean_println ( args ... ; kwargs ... ) Add rank and size information to the printed statement source # FluxMPI.local_rank \u2014 Function . local_rank () Get the rank of the process. source # FluxMPI.total_workers \u2014 Function . total_workers () Get the total number of workers. source MPIExtensions \u00a4 Blocking Communication Wrappers \u00a4 # FluxMPI.MPIExtensions.allreduce! \u2014 Function . allreduce! ( v , op , comm ) Perform MPI.Allreduce! ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled. source # FluxMPI.MPIExtensions.bcast! \u2014 Function . bcast! ( v , op , comm ) Perform MPI.Bcast! ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled. source # FluxMPI.MPIExtensions.reduce! \u2014 Function . reduce! ( v , op , comm ) Perform MPI.Reduce! ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled. source Non-Blocking Communication \u00a4 # FluxMPI.MPIExtensions.Iallreduce! \u2014 Function . Iallreduce! ( sendbuf , recvbuf , op , comm ) Iallreduce! ( sendrecvbuf , op , comm ) Performs non-blocking elementwise reduction using the operator op on the buffer sendbuf . sendbuf can also be a scalar, in which case recvbuf will be a value of the same type. recvbuf and an MPI_Request object are returned. The value in recvbuf is only valid after the request has been completed. ( MPI.Wait! ) Warning OpenMPI doesn't support Iallreduce! with CUDA. See this issue . source # FluxMPI.MPIExtensions.Ibcast! \u2014 Function . Ibcast! ( buf , root , comm ) Non-blocking broadcast of the buffer buf to all processes in comm . recvbuf and an MPI_Request object are returned. The value in recvbuf is only valid after the request has been completed. ( MPI.Wait! ) source Optimization \u00a4 # FluxMPI.DistributedOptimizer \u2014 Type . DistributedOptimizer ( optimizer ) Wrap the optimizer in a DistributedOptimizer . Before updating the parameters, this adds the gradients across the processes using non-blocking Allreduce Arguments optimizer : An Optimizer compatible with the Optimisers.jl package Note Remember to scale the loss function by 1 / total_workers() to ensure that the gradients are correctly averaged source # FluxMPI.allreduce_gradients \u2014 Function . allreduce_gradients ( gs :: NamedTuple ; on_gpu :: Bool = CUDA . functional ()) Allreduce the gradients. This uses a non-blocking API which will be efficient for large containers of multiple parameter arrays. Arguments gs : A NamedTuple of gradients Keyword Arguments on_gpu : Specify if the gradients are on GPU. Defaults to CUDA.functional() Returns Allreduce d NamedTuple of gradients source Synchronization \u00a4 # FluxMPI.synchronize! \u2014 Function . synchronize! ( x :: NamedTuple ; root_rank :: Integer = 0 ) synchronize! ( x :: Tuple ; root_rank :: Integer = 0 ) synchronize! ( x :: AbstractArray ; root_rank :: Integer = 0 ) synchronize! ( x :: ComponentArray ; root_rank :: Integer = 0 ) synchronize! ( x :: Nothing ; root_rank :: Integer = 0 ) synchronize! ( x :: Symbol ; root_rank :: Integer = 0 ) synchronize! ( x :: Leaf ; root_rank :: Integer = 0 ) synchronize! ( x :: Number ; root_rank :: Integer = 0 ) Synchronize x across all processes. source","title":"API Reference"},{"location":"api/#data-helpers","text":"# FluxMPI.DistributedDataContainer \u2014 Type . DistributedDataContainer ( data ) data must be compatible with MLUtils interface. The returned container is compatible with MLUtils interface and is used to partition the dataset across the available processes. source","title":"Data Helpers"},{"location":"api/#general-functions","text":"# FluxMPI.Init \u2014 Function . Init (; gpu_devices :: Union { Nothing , Vector { Int }} = nothing , verbose :: Bool = false ) Setup FluxMPI . If GPUs are available and CUDA is functional, each rank is allocated a GPU in a round-robin fashion. If calling this function, no need to call MPI.Init first. source # FluxMPI.Initialized \u2014 Function . Initialized () Has FluxMPI been initialized? source # FluxMPI.clean_print \u2014 Function . clean_print ( args ... ; kwargs ... ) Add rank and size information to the printed statement source # FluxMPI.clean_println \u2014 Function . clean_println ( args ... ; kwargs ... ) Add rank and size information to the printed statement source # FluxMPI.local_rank \u2014 Function . local_rank () Get the rank of the process. source # FluxMPI.total_workers \u2014 Function . total_workers () Get the total number of workers. source","title":"General Functions"},{"location":"api/#mpiextensions","text":"","title":"MPIExtensions"},{"location":"api/#blocking-communication-wrappers","text":"# FluxMPI.MPIExtensions.allreduce! \u2014 Function . allreduce! ( v , op , comm ) Perform MPI.Allreduce! ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled. source # FluxMPI.MPIExtensions.bcast! \u2014 Function . bcast! ( v , op , comm ) Perform MPI.Bcast! ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled. source # FluxMPI.MPIExtensions.reduce! \u2014 Function . reduce! ( v , op , comm ) Perform MPI.Reduce! ensuring that CuArrays are safely transfered to CPU if CUDA-aware MPI is unavailable/disabled. source","title":"Blocking Communication Wrappers"},{"location":"api/#non-blocking-communication","text":"# FluxMPI.MPIExtensions.Iallreduce! \u2014 Function . Iallreduce! ( sendbuf , recvbuf , op , comm ) Iallreduce! ( sendrecvbuf , op , comm ) Performs non-blocking elementwise reduction using the operator op on the buffer sendbuf . sendbuf can also be a scalar, in which case recvbuf will be a value of the same type. recvbuf and an MPI_Request object are returned. The value in recvbuf is only valid after the request has been completed. ( MPI.Wait! ) Warning OpenMPI doesn't support Iallreduce! with CUDA. See this issue . source # FluxMPI.MPIExtensions.Ibcast! \u2014 Function . Ibcast! ( buf , root , comm ) Non-blocking broadcast of the buffer buf to all processes in comm . recvbuf and an MPI_Request object are returned. The value in recvbuf is only valid after the request has been completed. ( MPI.Wait! ) source","title":"Non-Blocking Communication"},{"location":"api/#optimization","text":"# FluxMPI.DistributedOptimizer \u2014 Type . DistributedOptimizer ( optimizer ) Wrap the optimizer in a DistributedOptimizer . Before updating the parameters, this adds the gradients across the processes using non-blocking Allreduce Arguments optimizer : An Optimizer compatible with the Optimisers.jl package Note Remember to scale the loss function by 1 / total_workers() to ensure that the gradients are correctly averaged source # FluxMPI.allreduce_gradients \u2014 Function . allreduce_gradients ( gs :: NamedTuple ; on_gpu :: Bool = CUDA . functional ()) Allreduce the gradients. This uses a non-blocking API which will be efficient for large containers of multiple parameter arrays. Arguments gs : A NamedTuple of gradients Keyword Arguments on_gpu : Specify if the gradients are on GPU. Defaults to CUDA.functional() Returns Allreduce d NamedTuple of gradients source","title":"Optimization"},{"location":"api/#synchronization","text":"# FluxMPI.synchronize! \u2014 Function . synchronize! ( x :: NamedTuple ; root_rank :: Integer = 0 ) synchronize! ( x :: Tuple ; root_rank :: Integer = 0 ) synchronize! ( x :: AbstractArray ; root_rank :: Integer = 0 ) synchronize! ( x :: ComponentArray ; root_rank :: Integer = 0 ) synchronize! ( x :: Nothing ; root_rank :: Integer = 0 ) synchronize! ( x :: Symbol ; root_rank :: Integer = 0 ) synchronize! ( x :: Leaf ; root_rank :: Integer = 0 ) synchronize! ( x :: Number ; root_rank :: Integer = 0 ) Synchronize x across all processes. source","title":"Synchronization"},{"location":"common_gotchas/","text":"CUDA-aware MPI \u00a4 Setup \u00a4 OpenMPI has extensive instructions on building CUDA-aware MPI . Next rebuild MPI.jl using these instructions . Additionally, make sure to set JULIA_CUDA_USE_MEMPOOL=none . Should you use CUDA-aware MPI? \u00a4 I would recommend not using this atm, since JULIA_CUDA_USE_MEMPOOL=none will severely slow down your code ( ~2-3x for most workloads I tested). Instead setup MPI.jl using you system provided MPI and set FLUXMPI_DISABLE_CUDAMPI_SUPPORT=true .","title":"Common Gotchas"},{"location":"common_gotchas/#cuda-aware-mpi","text":"","title":"CUDA-aware MPI"},{"location":"common_gotchas/#setup","text":"OpenMPI has extensive instructions on building CUDA-aware MPI . Next rebuild MPI.jl using these instructions . Additionally, make sure to set JULIA_CUDA_USE_MEMPOOL=none .","title":"Setup"},{"location":"common_gotchas/#should-you-use-cuda-aware-mpi","text":"I would recommend not using this atm, since JULIA_CUDA_USE_MEMPOOL=none will severely slow down your code ( ~2-3x for most workloads I tested). Instead setup MPI.jl using you system provided MPI and set FLUXMPI_DISABLE_CUDAMPI_SUPPORT=true .","title":"Should you use CUDA-aware MPI?"},{"location":"guide/","text":"Guide to integrating FluxMPI into your code \u00a4 There are essentially 6 main steps to remember: Initialize FluxMPI FluxMPI.Init() . Sync Model Parameters and States FluxMPI.synchronize!(ps; root_rank) . Use DistributedDataContainer to distribute your data evenly across the processes. (Of course an alternative is to just manually partition your data.) Wrap the optimizer in DistributedOptimizer or call allreduce_gradients(gs::NamedTuple) before eveery Optimisers.update . Sync the optimizer state across the processes FluxMPI.synchronize!(opt_state; root_rank) . Change logging code to check for local_rank == 0. Finally, start the code using mpiexecjl -n <np> julia --project=. <filename>.jl","title":"Usage Guide"},{"location":"guide/#guide-to-integrating-fluxmpi-into-your-code","text":"There are essentially 6 main steps to remember: Initialize FluxMPI FluxMPI.Init() . Sync Model Parameters and States FluxMPI.synchronize!(ps; root_rank) . Use DistributedDataContainer to distribute your data evenly across the processes. (Of course an alternative is to just manually partition your data.) Wrap the optimizer in DistributedOptimizer or call allreduce_gradients(gs::NamedTuple) before eveery Optimisers.update . Sync the optimizer state across the processes FluxMPI.synchronize!(opt_state; root_rank) . Change logging code to check for local_rank == 0. Finally, start the code using mpiexecjl -n <np> julia --project=. <filename>.jl","title":"Guide to integrating FluxMPI into your code"},{"location":"examples/lux/","text":"Usage example with Lux.jl \u00a4 Lux.jl is a deep learning framework which provides a functional design to implement neural networks in Julia. If you are not familiar with Lux, first check-out this tutorial before proceeding. Step 0: Import the necessary packages \u00a4 Tip You can install these packages using import Pkg; Pkg.add.([\"CUDA\", \"Optimisers\", \"Lux\", \"Random\", \"Zygote\"]) import CUDA , Optimisers , FluxMPI , Lux , Random , Zygote Step 1: Initialize FluxMPI. Not doing this will segfault your code \u00a4 FluxMPI . Init () CUDA . allowscalar ( false ) Step 2: Sync Model Parameters and States \u00a4 model = Lux . Chain ( Lux . Dense ( 1 , 256 , tanh ), Lux . Dense ( 256 , 512 , tanh ), Lux . Dense ( 512 , 256 , tanh ), Lux . Dense ( 256 , 1 )) rng = Random . default_rng () Random . seed! ( rng , local_rank ()) ps , st = Lux . setup ( rng , model ) .|> Lux . gpu ps = FluxMPI . synchronize! ( ps ; root_rank = 0 ) st = FluxMPI . synchronize! ( st ; root_rank = 0 ) Step 3: Ensure data is properly partitioned \u00a4 It is the user's responsibility to partition the data across the processes. In this case, we are training on a total of 16 * <np> samples. Instead of manually partitioning the data, we can use DistributedDataContainer to partition the data. x = rand ( rng , 1 , 16 ) |> Lux . gpu y = x .^ 2 Step 4: Wrap the optimizer in DistributedOptimizer \u00a4 Remember to scale the learning rate by the number of workers total_workers . opt = FluxMPI . DistributedOptimizer ( Optimisers . ADAM ( 0.001f0 )) st_opt = Optimisers . setup ( opt , ps ) loss ( p ) = sum ( abs2 , model ( x , p , st )[ 1 ] .- y ) Step 5: Synchronize the optimizer state \u00a4 st_opt = FluxMPI . synchronize! ( st_opt ; root_rank = 0 ) Step 6: Train your model \u00a4 Remember to print using clean_println or clean_print . gs_ = Zygote . gradient ( loss , ps )[ 1 ] Optimisers . update ( st_opt , ps , gs_ ) t1 = time () for epoch in 1 : 100 global ps , st_opt l , back = Zygote . pullback ( loss , ps ) FluxMPI . clean_println ( \"Epoch $epoch : Loss $l \" ) gs = back ( one ( l ))[ 1 ] st_opt , ps = Optimisers . update ( st_opt , ps , gs ) end FluxMPI . clean_println ( time () - t1 ) Run the code using mpiexecjl -n 3 julia --project=. <filename>.jl . Complete Script \u00a4 import CUDA , FluxMPI , Lux , Optimisers , Random , Zygote FluxMPI . Init () CUDA . allowscalar ( false ) model = Lux . Chain ( Lux . Dense ( 1 , 256 , tanh ), Lux . Dense ( 256 , 512 , tanh ), Lux . Dense ( 512 , 256 , tanh ), Lux . Dense ( 256 , 1 )) rng = Random . default_rng () Random . seed! ( rng , FluxMPI . local_rank ()) ps , st = Lux . setup ( rng , model ) .|> Lux . gpu ps = FluxMPI . synchronize! ( ps ; root_rank = 0 ) st = FluxMPI . synchronize! ( st ; root_rank = 0 ) x = rand ( rng , 1 , 16 ) |> Lux . gpu y = x .^ 2 opt = FluxMPI . DistributedOptimizer ( Optimisers . ADAM ( 0.001f0 )) st_opt = Optimisers . setup ( opt , ps ) loss ( p ) = sum ( abs2 , model ( x , p , st )[ 1 ] .- y ) st_opt = FluxMPI . synchronize! ( st_opt ; root_rank = 0 ) gs_ = Zygote . gradient ( loss , ps )[ 1 ] Optimisers . update ( st_opt , ps , gs_ ) t1 = time () for epoch in 1 : 100 global ps , st_opt l , back = Zygote . pullback ( loss , ps ) FluxMPI . clean_println ( \"Epoch $epoch : Loss $l \" ) gs = back ( one ( l ))[ 1 ] st_opt , ps = Optimisers . update ( st_opt , ps , gs ) end FluxMPI . clean_println ( time () - t1 )","title":"Usage with Lux"},{"location":"examples/lux/#usage-example-with-luxjl","text":"Lux.jl is a deep learning framework which provides a functional design to implement neural networks in Julia. If you are not familiar with Lux, first check-out this tutorial before proceeding.","title":"Usage example with Lux.jl"},{"location":"examples/lux/#step-0-import-the-necessary-packages","text":"Tip You can install these packages using import Pkg; Pkg.add.([\"CUDA\", \"Optimisers\", \"Lux\", \"Random\", \"Zygote\"]) import CUDA , Optimisers , FluxMPI , Lux , Random , Zygote","title":"Step 0: Import the necessary packages"},{"location":"examples/lux/#step-1-initialize-fluxmpi-not-doing-this-will-segfault-your-code","text":"FluxMPI . Init () CUDA . allowscalar ( false )","title":"Step 1: Initialize FluxMPI. Not doing this will segfault your code"},{"location":"examples/lux/#step-2-sync-model-parameters-and-states","text":"model = Lux . Chain ( Lux . Dense ( 1 , 256 , tanh ), Lux . Dense ( 256 , 512 , tanh ), Lux . Dense ( 512 , 256 , tanh ), Lux . Dense ( 256 , 1 )) rng = Random . default_rng () Random . seed! ( rng , local_rank ()) ps , st = Lux . setup ( rng , model ) .|> Lux . gpu ps = FluxMPI . synchronize! ( ps ; root_rank = 0 ) st = FluxMPI . synchronize! ( st ; root_rank = 0 )","title":"Step 2: Sync Model Parameters and States"},{"location":"examples/lux/#step-3-ensure-data-is-properly-partitioned","text":"It is the user's responsibility to partition the data across the processes. In this case, we are training on a total of 16 * <np> samples. Instead of manually partitioning the data, we can use DistributedDataContainer to partition the data. x = rand ( rng , 1 , 16 ) |> Lux . gpu y = x .^ 2","title":"Step 3: Ensure data is properly partitioned"},{"location":"examples/lux/#step-4-wrap-the-optimizer-in-distributedoptimizer","text":"Remember to scale the learning rate by the number of workers total_workers . opt = FluxMPI . DistributedOptimizer ( Optimisers . ADAM ( 0.001f0 )) st_opt = Optimisers . setup ( opt , ps ) loss ( p ) = sum ( abs2 , model ( x , p , st )[ 1 ] .- y )","title":"Step 4: Wrap the optimizer in DistributedOptimizer"},{"location":"examples/lux/#step-5-synchronize-the-optimizer-state","text":"st_opt = FluxMPI . synchronize! ( st_opt ; root_rank = 0 )","title":"Step 5: Synchronize the optimizer state"},{"location":"examples/lux/#step-6-train-your-model","text":"Remember to print using clean_println or clean_print . gs_ = Zygote . gradient ( loss , ps )[ 1 ] Optimisers . update ( st_opt , ps , gs_ ) t1 = time () for epoch in 1 : 100 global ps , st_opt l , back = Zygote . pullback ( loss , ps ) FluxMPI . clean_println ( \"Epoch $epoch : Loss $l \" ) gs = back ( one ( l ))[ 1 ] st_opt , ps = Optimisers . update ( st_opt , ps , gs ) end FluxMPI . clean_println ( time () - t1 ) Run the code using mpiexecjl -n 3 julia --project=. <filename>.jl .","title":"Step 6: Train your model"},{"location":"examples/lux/#complete-script","text":"import CUDA , FluxMPI , Lux , Optimisers , Random , Zygote FluxMPI . Init () CUDA . allowscalar ( false ) model = Lux . Chain ( Lux . Dense ( 1 , 256 , tanh ), Lux . Dense ( 256 , 512 , tanh ), Lux . Dense ( 512 , 256 , tanh ), Lux . Dense ( 256 , 1 )) rng = Random . default_rng () Random . seed! ( rng , FluxMPI . local_rank ()) ps , st = Lux . setup ( rng , model ) .|> Lux . gpu ps = FluxMPI . synchronize! ( ps ; root_rank = 0 ) st = FluxMPI . synchronize! ( st ; root_rank = 0 ) x = rand ( rng , 1 , 16 ) |> Lux . gpu y = x .^ 2 opt = FluxMPI . DistributedOptimizer ( Optimisers . ADAM ( 0.001f0 )) st_opt = Optimisers . setup ( opt , ps ) loss ( p ) = sum ( abs2 , model ( x , p , st )[ 1 ] .- y ) st_opt = FluxMPI . synchronize! ( st_opt ; root_rank = 0 ) gs_ = Zygote . gradient ( loss , ps )[ 1 ] Optimisers . update ( st_opt , ps , gs_ ) t1 = time () for epoch in 1 : 100 global ps , st_opt l , back = Zygote . pullback ( loss , ps ) FluxMPI . clean_println ( \"Epoch $epoch : Loss $l \" ) gs = back ( one ( l ))[ 1 ] st_opt , ps = Optimisers . update ( st_opt , ps , gs ) end FluxMPI . clean_println ( time () - t1 )","title":"Complete Script"}]}